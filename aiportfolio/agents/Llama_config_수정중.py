import transformers
import torch
import gc
import os

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€)
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# ì „ì—­ íŒŒì´í”„ë¼ì¸ ìºì‹œ
_pipeline_cache = None

def prepare_pipeline_obj():
    """
    íŒŒì´í”„ë¼ì¸ì„ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    global _pipeline_cache
    
    if _pipeline_cache is not None:
        print("âœ… ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©")
        return _pipeline_cache
    
    print("ğŸ”„ ìƒˆ íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
    
    # ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    print(f"ğŸ§¹ ì •ë¦¬ ì „ GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
    
    # ì–‘ìí™” ì—†ì´ float16ë§Œ ì‚¬ìš©
    _pipeline_cache = transformers.pipeline(
        "text-generation",
        model="meta-llama/Meta-Llama-3-8B-Instruct",
        model_kwargs={
            "torch_dtype": torch.float16,
            "low_cpu_mem_usage": True,
        },
        device_map="auto",
    )
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
    
    return _pipeline_cache


def cleanup_pipeline():
    global _pipeline_cache
    
    if _pipeline_cache is not None:
        print("ğŸ§¹ íŒŒì´í”„ë¼ì¸ ë©”ëª¨ë¦¬ í•´ì œ ì¤‘...")
        del _pipeline_cache
        _pipeline_cache = None
        gc.collect()
        torch.cuda.empty_cache()
        print("âœ… íŒŒì´í”„ë¼ì¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")


def chat_with_llama3(pipeline_obj, system_prompt, user_prompt):
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    prompt = pipeline_obj.tokenizer.apply_chat_template(
        messages,
        tokenize=False, 
        add_generation_prompt=True 
    )

    eos_tokens = [
        pipeline_obj.tokenizer.eos_token_id, 
        pipeline_obj.tokenizer.convert_tokens_to_ids("```"),
        pipeline_obj.tokenizer.convert_tokens_to_ids("]"),
        pipeline_obj.tokenizer.convert_tokens_to_ids("}")
    ]
    
    with torch.no_grad():
        outputs = pipeline_obj(
            prompt,
            max_new_tokens=128,  # 256 â†’ 128ë¡œ ë”ìš± ì¤„ì„
            do_sample=True,
            temperature=0.3,
            top_p=0.8,
            return_full_text=False,
            eos_token_id=eos_tokens,
            pad_token_id=pipeline_obj.tokenizer.eos_token_id
        )

    torch.cuda.empty_cache()
    
    return outputs[0]["generated_text"].strip()